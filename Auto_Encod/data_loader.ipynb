{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40289e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# --- 1. DATASET POUR LE D√âBRUITAGE (MNIST) ---\n",
    "# Utilisation de MNIST pour un entra√Ænement ultra-rapide\n",
    "(x_train_mnist, _), (x_test_mnist, _) = tf.keras.datasets.mnist.load_data()\n",
    "x_train_mnist = x_train_mnist.astype('float32') / 255.\n",
    "x_test_mnist = x_test_mnist.astype('float32') / 255.\n",
    "# On ajoute une dimension pour le canal (28, 28, 1)\n",
    "x_train_mnist = np.expand_dims(x_train_mnist, -1)\n",
    "x_test_mnist = np.expand_dims(x_test_mnist, -1)\n",
    "\n",
    "\n",
    "# --- 2. DATASET POUR LA COLORISATION (FLOWERS 102) ---\n",
    "# Chargement via TFDS (Plus lourd, n√©cessite une connexion internet)\n",
    "# Si Flowers est trop lourd, vous pouvez swapper avec 'cifar10'\n",
    "ds_flowers, info_flowers = tfds.load('oxford_flowers102', with_info=True, as_supervised=True)\n",
    "# Note : ds_flowers est un dictionnaire contenant 'train', 'test' et 'validation'\n",
    "\n",
    "\n",
    "# --- 3. DATASET POUR LA SUPER-R√âSOLUTION (STL-10 ou CIFAR-10) ---\n",
    "# On utilise CIFAR-10 ici pour la compatibilit√©, STL-10 est aussi possible via tfds\n",
    "(x_train_sr, _), (x_test_sr, _) = tf.keras.datasets.cifar10.load_data()\n",
    "x_train_sr = x_train_sr.astype('float32') / 255.\n",
    "x_test_sr = x_test_sr.astype('float32') / 255.\n",
    "\n",
    "\n",
    "# --- 4. DATASET POUR LA D√âTECTION D'ANOMALIES (CREDIT CARD FRAUD) ---\n",
    "# Lien de t√©l√©chargement : https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud\n",
    "# Assurez-vous que le fichier 'creditcard.csv' est dans votre dossier racine\n",
    "try:\n",
    "    df_fraud = pd.read_csv('creditcard.csv')\n",
    "    \n",
    "    # 1. On s√©pare les normales des fraudes\n",
    "    df_normal = df_fraud[df_fraud['Class'] == 0].drop(['Class', 'Time'], axis=1)\n",
    "    df_anomaly = df_fraud[df_fraud['Class'] == 1].drop(['Class', 'Time'], axis=1)\n",
    "    \n",
    "    # 2. Pr√©paration de l'entra√Ænement (Uniquement sur des transactions normales)\n",
    "    # On prend 50 000 normales pour l'entra√Ænement\n",
    "    x_normal_train, x_normal_test = train_test_split(\n",
    "        df_normal.sample(n=50000, random_state=42), \n",
    "        test_size=0.2, \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # 3. Normalisation (On calibre le scaler sur le train normal UNIQUEMENT)\n",
    "    scaler = StandardScaler()\n",
    "    x_train_anomaly = scaler.fit_transform(x_normal_train)\n",
    "    x_test_normal = scaler.transform(x_normal_test)\n",
    "    \n",
    "    # 4. Pr√©paration du set de FRAUDES pour le test final\n",
    "    # On normalise les fraudes avec le m√™me scaler\n",
    "    x_test_fraud = scaler.transform(df_anomaly)\n",
    "    \n",
    "    print(f\"‚úÖ Dataset charg√©.\")\n",
    "    print(f\"üè† Transactions normales pour l'entra√Ænement : {x_train_anomaly.shape[0]}\")\n",
    "    print(f\"üö® Transactions frauduleuses pour le test final : {x_test_fraud.shape[0]}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ö†Ô∏è Fichier 'creditcard.csv' non trouv√©.\")\n",
    "\n",
    "# --- R√âSUM√â DES DIMENSIONS ---\n",
    "print(f\"\\nüìä Denoising (MNIST): {x_train_mnist.shape}\")\n",
    "print(f\"üìä Super-Res (CIFAR): {x_train_sr.shape}\")\n",
    "print(f\"üìä Anomaly (Fraud): {x_train_anomaly.shape if 'x_train_anomaly' in locals() else 'N/A'}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
